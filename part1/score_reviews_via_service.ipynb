{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Existing Machine Learning Services\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part1/score_reviews_via_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain labelled reviews\n",
    "\n",
    "In order to test any of the sentiment analysis APIs, we need a labelled dataset of reviews and their sentiment polarity. We'll use NLTK to download the movie_reviews corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\tvanderm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The files in movie_reviews have already been divided into two sets: positive ('pos') and negative ('neg'), so we can load the raw text of the reviews into two lists, one for each polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# extract words from reviews, pair with label\n",
    "\n",
    "reviews_pos = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    review = movie_reviews.raw(fileid)\n",
    "    reviews_pos.append(review)\n",
    "\n",
    "reviews_neg = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    review = movie_reviews.raw(fileid)\n",
    "    reviews_neg.append(review)\n",
    "    \n",
    "print(len(reviews_pos))\n",
    "print(len(reviews_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'moviemaking is a lot like being the general manager of an nfl team in the post-salary cap era -- you\\'ve got to know how to allocate your resources . \\nevery dollar spent on a free-agent defensive tackle is one less dollar than you can spend on linebackers or safeties or centers . \\nin the nfl , this leads to teams like the detroit lions , who boast a superstar running back with a huge contract , but can only field five guys named herb to block for him . \\nin the movies , you end up with films like \" spawn \" , with a huge special-effects budget but not enough money to hire any recognizable actors . \\njackie chan is the barry sanders of moviemaking . \\nhe spins and darts across the screen like sanders cutting back through the defensive line . \\nwatching jackie in operation condor as he drives his motorcycle through the crowded streets of madrid , fleeing an armada of pursuers in identical black compact cars , is reminiscent of sanders running for daylight with the chicago bears in hot pursuit , except that sanders doesn\\'t have to worry about rescuing runaway baby carriages . \\nbut like the lions star , jackie doesn\\'t have anybody to block for him . \\nalmost every cent that\\'s invested in a jackie chan movie goes for stunts , and as chan does his own stunts , the rest of the money goes to pay his hospital bills . \\nthis leaves about 75 cents to pay for things like directors ( chan directs ) , scripts and dubbing and supporting characters , not to mention the hideous title sequence . \\nthis also explains why the movie was shot in odd places like morocco and spain . \\n ( chan\\'s first release in this country , \" rumble in the bronx \" , was supposedly set in new york , but was filmed in vancouver , and in the chase scenes the canadian rockies are clearly visible . ) \\nheck , jackie doesn\\'t even have enough money for a haircut , looks like , much less a personal hairstylist . \\nin condor , chan plays the same character he\\'s always played , himself , a mixture of bruce lee and tim allen , a master of both kung-fu and slapstick-fu . \\njackie is sent by the un to retrieve a cache of lost nazi gold in the north african desert , and is chased by a horde of neo-nazi sympathizers and two stereotypical arabs ( one of the things i like about jackie chan movies : no political correctness ) . \\nhe is joined by three women , who have little to do except scream , \" jackie , save us ! \" , and misuse firearms . \\nthe villain is an old nazi whose legs were broken in the secret base so that he has to be carried everywhere , and he\\'s more pathetic than evil . \\nen route , we have an extended motorcycle chase scene , a hilarious fight in the moroccan version of motel 6 with the neo-nazis , and two confrontations with savage natives . \\nonce at the secret desert base , there is a long chop-socky sequence , followed by the film\\'s centerpiece , a wind-tunnel fight that\\'s even better than the one in face/off . \\nthis is where the money was spent , on well-choreographed kung-fu sequences , on giant kevlar hamster balls , on smashed-up crates of bananas , and on scorpions . \\nignore the gaping holes in the plot ( how , exactly , if the villain\\'s legs were broken , did he escape from the secret nazi base , and why didn\\'t he take the key with him ? ) . \\ndon\\'t worry about the production values , or what , exactly , the japanese girl was doing hitchhiking across the sahara . \\njust go see the movie . \\noperation condor has pretentions of being a \" raiders of the lost ark \" knockoff , but one wonders what jackie could do with the raiders franchise blocking for him -- with a lawrence kazdan screenplay , a john williams score , spielberg directing and george lucas producing , condor might be an a+ movie . \\nhowever , you\\'ve got to go with what you\\'ve got , and what you\\'ve got in jackie chan is something special -- a talent that mainstream hollywood should , could , and ought to utilize . \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what one review looks like\n",
    "print(len(reviews_pos[4]))\n",
    "reviews_pos[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the scoring API\n",
    "\n",
    "Fill in this function with code that connects to one of these APIs, and uses it to score a single review:\n",
    "\n",
    "* [Amazon Comprehend: Detect Sentiment](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectSentiment.html)\n",
    "* [Google Natural Language: Analyzing Sentiment](https://cloud.google.com/natural-language/docs/analyzing-sentiment)\n",
    "* [Azure Cognitive Services: Sentiment Analysis](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-sentiment-analysis)\n",
    "* [Algorithmia: Sentiment Analysis](https://algorithmia.com/algorithms/nlp/SentimentAnalysis)\n",
    "\n",
    "Your function must return either 'pos' or 'neg', so you'll need to make some decisions about how to map the results of the API call to one of these values. For example, Amazon Comprehend can return \"NEUTRAL\" or \"MIXED\" for the Sentiment -- if this happens, you may with to inspect the numeric values under the SentimentScore to see whether it leans toward positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "# progress bar\n",
    "from ipypb import track, irange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectSentiment\n",
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"161\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Sun, 27 Sep 2020 04:19:54 GMT\",\n",
      "            \"x-amzn-requestid\": \"920f7dc2-d8a3-4e7b-806b-65ae670012e0\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"920f7dc2-d8a3-4e7b-806b-65ae670012e0\",\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"Sentiment\": \"NEUTRAL\",\n",
      "    \"SentimentScore\": {\n",
      "        \"Mixed\": 0.00021913634554948658,\n",
      "        \"Negative\": 0.162128284573555,\n",
      "        \"Neutral\": 0.7376415133476257,\n",
      "        \"Positive\": 0.10001111775636673\n",
      "    }\n",
      "}\n",
      "End of DetectSentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test - from https://docs.aws.amazon.com/code-samples/latest/catalog/python-comprehend-DetectSentiment.py.html\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectSentiment')\n",
    "print(json.dumps(comprehend.detect_sentiment(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectSentiment\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep hearing about other students hitting a 5K limit on Amazon Comprehend. \n",
    "# rather than truncate the review I'm going to first try removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import NLTKWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'both', 'below', 'that', 'under', 'while', 'herself', 's', 'was', 'himself', 'off', 'whom', 'does', 'he', 'down', \"you've\", 'yourself', 'with', 'yours', 'above', 'm', 've', 'few', 'other', 'here', 'she', 'until', 'who', 'or', 'so', 'had', 'any', 'further', \"should've\", 'those', 'nor', 'are', 'is', 'between', 'own', 'our', 're', 'his', 'from', 'my', \"you'd\", 'their', 'has', 'most', 'y', 'same', 'where', 'ourselves', 'against', 'no', 'did', 'what', 'be', 'because', 'me', 'its', \"you're\", 'him', 'a', 'just', 'we', 'itself', \"it's\", 'each', 'you', 'then', 'there', 'd', 'too', 'have', 'as', 'such', 'do', 'should', 'it', 'during', \"that'll\", 'her', 'hers', 'they', 'up', 'myself', 'which', 'your', 'more', 'in', 'can', 'over', 'not', 'don', 'when', 'at', 'after', 'am', 'into', \"you'll\", 'been', 'but', 'of', 'theirs', 'some', 't', \"don't\", 'how', 'll', '``', 'if', 'themselves', 'once', 'out', 'on', 'all', 'having', 'again', 'an', 'were', 'ma', 'the', 'by', 'why', 'this', 'ours', 'will', 'and', 'yourselves', 'very', 'than', 'to', 'about', 'for', 'before', 'i', 'them', 'o', 'only', 'now', 'these', 'doing', 'through', 'being', \"she's\"}\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "# some of the stopwords might be useful, like the negative ones\n",
    "neg_words = {'mightn', \"mightn't\", 'mustn', \"mustn't\",  'needn',  \"needn't\",  'shan',  \"shan't\",  'shouldn',  \"shouldn't\",  'wasn',  \"wasn't\",  'weren',  \"weren't\",  'won',  \"won't\",  'wouldn',  \"wouldn't\",  'ain',  'aren',  \"aren't\",  'couldn',  \"couldn't\",  'didn',  \"didn't\",  'doesn',  \"doesn't\",  'hadn',  \"hadn't\",  'hasn',  \"hasn't\",  'haven',  \"haven't\",  'isn',  \"isn't\"}\n",
    "stops = set(stopwords.words(\"english\")+ ['``']) - neg_words \n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKWordTokenizer()\n",
    "# test it\n",
    "#tokenizer.tokenize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review string length: 3255\n",
      "Review string length without stops: 2334\n"
     ]
    }
   ],
   "source": [
    "# try removing stop words to get past Comprehend's 5K character limit\n",
    "# does tokenizing help?\n",
    "print(\"Raw review string length:\", len(review))\n",
    "print(\"Review string length without stops:\", len(remove_stopwords(review, stops)))\n",
    "# works a little, maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"two party guys bob heads haddaway 's dance hit love ? getting trouble nightclub nightclub . 's barely enough sustain three-minute _saturday_night_live_ skit , _snl_ producer lorne michaels , _clueless_ creator amy heckerling , paramount pictures saw something late night television institution 's recurring roxbury guys sketch would presumably make good feature . emphasis word presumably . _a_night_at_the_roxbury_ takes already-thin concept tediously stretches far beyond breaking point -- viewers ' patience levels . first five minutes _roxbury_ play much like one original roxbury guys skits . love ? blaring soundtrack , brotherly duo doug steve butabi ( chris kattan ferrell ) bob heads , scope hotties clubs , bump select violent pelvic thrusts . one crucial difference , however -- guys speak . little fact used justification film 's existence , butabis ' newfound capacity speech would open whole new set doors characters . doors opened director john fortenberry screenwriters steve koren , ferrell , kattan new , 's sure , lead comic dead ends . story per se , loosely structured linked series subplots . brothers literally run ( , rather , get run , car ) richard grieco _21_jump_street_ fame , gain entrance exclusive roxbury club . , meet hotshot club owner ( chazz palminteri , conspicuously uncredited -- blame ? ) , takes interest idea . meanwhile , bros ' overbearing father ( dan hedaya ) wants stop clubbing . doug refuses dimwitted steve obeys father , rift created two . narrative messiness _roxbury_ would forgivable went slightest bit funny , virtually none . assembled press audience mostly sat stonily silent throughout entire film , one big exception big laugh near end . alas , joke -- rather lazy takeoff _jerry_maguire_ -- strike chord people seen film . granted , lot people _have_ seen _jerry_maguire_ , fact film 's best joke completely dependent one 's familiarity another film says lot _roxbury_ 's lack inspiration . lack inspiration traced back insipid characters . like many skits current incarnation _saturday_night_live_ , roxbury guys one-joke sketch never suggests characters enough comic life survive outside sketch context . watching one roxbury skits snl , come away characters : bob heads love ? , bump unsuspecting women , . . . 's . watching _a_night_at_the_roxbury_ , 'll left exactly .\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(review, stopwords=[]):\n",
    "    tokenizer = NLTKWordTokenizer()\n",
    "    return \" \".join([x.strip() for x in tokenizer.tokenize(review) if x.strip() != \"\" and x.strip() not in stopwords])\n",
    "\n",
    "remove_stopwords(review, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(review, stopwords=[]):\n",
    "    tokenizer = NLTKWordTokenizer()\n",
    "    return \" \".join([x for x in tokenizer.tokenize(review) if x.strip() not in (\"\", \"``\") and x not in stopwords])\n",
    "    \n",
    "\n",
    "def score_review(review, aws_region='us-east-1', stopwords=[], force_remove_stops=False):\n",
    "    \"\"\"calls Comprehend service for review. Returns either 'positive' or 'negative'\n",
    "    --review ==> (str) movie review text.\n",
    "    --aws_region ==> (str) preferred host region for AWS, e.g., 'us-east-1' \n",
    "    --stopwords ==> list of words to remove from string, if necessary \n",
    "                    (will automatically attempt to remove if review exceeds Comprehend's 5K character limit)\n",
    "    --remove_stops ==> If True, will remove stopwords from review string even if below\n",
    "                        Comprehend's 5K character limit\n",
    "    \"\"\"\n",
    "    if force_remove_stops:\n",
    "        review = remove_stopwords(review, stopwords)\n",
    "        \n",
    "    comprehend = boto3.client(service_name='comprehend', region_name=aws_region)\n",
    "    try:\n",
    "        response = comprehend.detect_sentiment(Text=review, LanguageCode='en')\n",
    "    except:  # still can't find where to import Comprehend's exceptions, like TextSizeLimitExceededException\n",
    "        # we likely ran up against the 5K character limit\n",
    "        review = remove_stopwords(review, stopwords)\n",
    "        # might still be too long\n",
    "        if len(review.encode('utf-8')) > 5000:\n",
    "            # now truncate it \n",
    "            review = review[:4999]\n",
    "        response = comprehend.detect_sentiment(Text=review, LanguageCode='en')\n",
    "    sentiment_final = response.get(\"Sentiment\").lower()\n",
    "    sentiment_scores = response.get(\"SentimentScore\")\n",
    "    # AWS Comprehend returns positive, negative, neutral and mixed\n",
    "    # but we only want pos/neg, so if final is Neutral or Mixed,\n",
    "    # instead use the highest score between Positive and Negative\n",
    "#     print(sentiment_final)\n",
    "    if sentiment_final not in [\"positive\", \"negative\"]:\n",
    "        pos_neg = [(key.lower(),value) for key, value in sentiment_scores.items() if key.lower() in [\"positive\", \"negative\"]]\n",
    "#         print(pos_neg)\n",
    "        sentiment_final = sorted(pos_neg, key=lambda x:x[1], reverse=True)[0][0]\n",
    "#         print(sentiment_final)\n",
    "\n",
    "    return sentiment_final.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using function without forced stopword removal: positive\n",
      "Using function with forced stopword removal: negative\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "tests = [reviews_pos[110]]\n",
    "for test in tests:\n",
    "    print(\"Using function without forced stopword removal:\", score_review(review, stopwords=stops, force_remove_stops=False))\n",
    "    print(\"Using function with forced stopword removal:\", score_review(review, stopwords=stops, force_remove_stops=True))\n",
    "    \n",
    "    \n",
    "# comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "# print(\"Without tokenization:\")\n",
    "# print(comprehend.detect_sentiment(Text=review, LanguageCode='en'))\n",
    "# print(\"With tokenization:\")\n",
    "# print(comprehend.detect_sentiment(Text=remove_stopwords(review, stopwords=stops), LanguageCode='en'))\n",
    "# print(\"****************\")\n",
    "# print(\"Using function without forced stopword removal:\", score_review(review, stopwords=stops, force_remove_stops=False))\n",
    "# print(\"Using function with forced stopword removal:\", score_review(review, stopwords=stops, force_remove_stops=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score each review\n",
    "\n",
    "Now, we can use the function you defined to score each of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"1000\" value=\"1000\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">1000/1000</span>\n",
       "<span class=\"Time-label\">[06:04<00:00, 0.36s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 1000/1000 [06:04<00:00, 0.36s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"1000\" value=\"1000\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">1000/1000</span>\n",
       "<span class=\"Time-label\">[05:50<00:00, 0.35s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 1000/1000 [05:50<00:00, 0.35s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_pos = []\n",
    "for review in track(reviews_pos):\n",
    "    result = score_review(review, stopwords=stops, force_remove_stops=True)\n",
    "    results_pos.append(result)\n",
    "\n",
    "results_neg = []\n",
    "for review in track(reviews_neg):\n",
    "    result = score_review(review, stopwords=stops, force_remove_stops=True)\n",
    "    results_neg.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n",
    "\n",
    "For each of our known positive reviews, we can count the number which our function scored as 'pos', and use this to calculate the % accuracy. We repeaty this for negative reviews, and also for overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 74.6% correct\n",
      "Negative reviews: 58.099999999999994% correct\n",
      "Overall accuracy: 66.35% correct\n"
     ]
    }
   ],
   "source": [
    "correct_pos = results_pos.count('positive')\n",
    "accuracy_pos = float(correct_pos) / len(results_pos)\n",
    "correct_neg = results_neg.count('negative')\n",
    "accuracy_neg = float(correct_neg) / len(results_neg)\n",
    "correct_all = correct_pos + correct_neg\n",
    "accuracy_all = float(correct_all) / (len(results_pos)+len(results_neg))\n",
    "\n",
    "print('Positive reviews: {}% correct'.format(accuracy_pos*100))\n",
    "print('Negative reviews: {}% correct'.format(accuracy_neg*100))\n",
    "print('Overall accuracy: {}% correct'.format(accuracy_all*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without forced stop word removal:\n",
    " - Positive reviews: 75.5% correct\n",
    " - Negative reviews: 56.8% correct\n",
    " - Overall accuracy: 66.14999999999999% correct\n",
    "    \n",
    "### with forced stop word removal:\n",
    " - Positive reviews: 74.6% correct\n",
    " - Negative reviews: 58.099999999999994% correct\n",
    " - Overall accuracy: 66.35% correct\n",
    " \n",
    " So a very slight gain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_mlmicro)",
   "language": "python",
   "name": "conda_mlmicro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
